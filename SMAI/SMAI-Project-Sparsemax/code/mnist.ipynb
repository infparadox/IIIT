{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-yDRSRGGOyCA"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Sparsemax(nn.Module):\n",
    "    def __init__(self, num_clusters, num_neurons_per_cluster):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_neurons_per_cluster = num_neurons_per_cluster\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        input_reshape = torch.zeros(input.size())\n",
    "        input_reshape = input.view(-1, self.num_clusters, self.num_neurons_per_cluster)\n",
    "        dim = 2\n",
    "        input_shift = input_reshape\n",
    "\n",
    "        z_sorted = torch.sort(input_shift, dim=dim, descending=True)[0]\n",
    "        input_size = input_shift.size()[dim]\n",
    "        range_values = Variable(torch.arange(1, input_size+1), requires_grad=False)\n",
    "        range_values = range_values.expand_as(z_sorted)\n",
    "\n",
    "        bound = Variable(torch.zeros(z_sorted.size()),requires_grad=False)\n",
    "        bound = 1 + torch.addcmul(bound, range_values, z_sorted)\n",
    "        cumsum_zs = torch.cumsum(z_sorted, dim)\n",
    "        is_gt = torch.gt(bound, cumsum_zs).type(torch.FloatTensor)\n",
    "        valid = Variable(torch.zeros(range_values.size()),requires_grad=False)\n",
    "        valid = torch.addcmul(valid, range_values, is_gt)\n",
    "        k_max = torch.max(valid, dim)[0]\n",
    "        zs_sparse = Variable(torch.zeros(z_sorted.size()),requires_grad=False)\n",
    "        zs_sparse = torch.addcmul(zs_sparse, is_gt, z_sorted)\n",
    "        sum_zs = (torch.sum(zs_sparse, dim) - 1)\n",
    "        taus = Variable(torch.zeros(k_max.size()),requires_grad=False)\n",
    "        taus = torch.addcdiv(taus, (torch.sum(zs_sparse, dim) - 1), k_max)\n",
    "        taus = torch.unsqueeze(taus,1)\n",
    "        taus_expanded = taus.expand_as(input_shift)\n",
    "        output = Variable(torch.zeros(input_reshape.size()))\n",
    "        output = torch.max(output, input_shift - taus_expanded)\n",
    "        return output.view(-1, self.num_clusters*self.num_neurons_per_cluster), zs_sparse,taus, is_gt\n",
    "\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.output = self.output.view(-1,self.num_clusters, self.num_neurons_per_cluster)\n",
    "        grad_output = grad_output.view(-1,self.num_clusters, self.num_neurons_per_cluster)\n",
    "        dim = 2\n",
    "        non_zeros = Variable(torch.ne(self.output, 0).type(torch.FloatTensor), requires_grad=False)\n",
    "        mask_grad = Variable(torch.zeros(self.output.size()), requires_grad=False)\n",
    "        mask_grad = torch.addcmul(mask_grad, non_zeros, grad_output)\n",
    "        sum_mask_grad = torch.sum(mask_grad, dim)\n",
    "        l1_norm_non_zeros = torch.sum(non_zeros, dim)\n",
    "        sum_v = Variable(torch.zeros(sum_mask_grad.size()), requires_grad=False)\n",
    "        sum_v = torch.addcdiv(sum_v, sum_mask_grad, l1_norm_non_zeros)\n",
    "        self.gradInput = Variable(torch.zeros(grad_output.size()))\n",
    "        self.gradInput = torch.addcmul(self.gradInput, non_zeros, grad_output - sum_v.expand_as(grad_output))\n",
    "        self.gradInput = self.gradInput.view(-1, self.num_clusters*self.num_neurons_per_cluster)\n",
    "        return self.gradInput\n",
    "\n",
    "class MultiLabelSparseMaxLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_clusters, num_neurons_per_cluster):\n",
    "        super(MultiLabelSparseMaxLoss, self).__init__()\n",
    "        self.num_clusters = num_clusters\n",
    "        self.num_neurons_per_cluster = num_neurons_per_cluster\n",
    "\n",
    "    def forward(self, input, zs_sparse, target, output_sparsemax, taus, is_gt):\n",
    "        self.output_sparsemax = output_sparsemax\n",
    "        input = input.view(-1, self.num_clusters, self.num_neurons_per_cluster)\n",
    "        self.target = target.view(-1, self.num_clusters, self.num_neurons_per_cluster)\n",
    "        batch_size = input.size(0)\n",
    "        dim = 2\n",
    "        target_times_input = torch.sum(self.target * input, dim)\n",
    "        target_inner_product = torch.sum(self.target * self.target, dim)\n",
    "        zs_squared = zs_sparse * zs_sparse\n",
    "        taus_squared = (taus * taus).expand_as(zs_squared)\n",
    "        taus_squared = taus_squared * is_gt\n",
    "        sum_input_taus = torch.sum(zs_squared - taus_squared, dim)\n",
    "        sparsemax_loss = - target_times_input + 0.5*sum_input_taus + 0.5*target_inner_product\n",
    "        sparsemax_loss = torch.sum(sparsemax_loss)/(batch_size * self.num_clusters)\n",
    "        return sparsemax_loss\n",
    "\n",
    "    def backward(self):\n",
    "        grad_output = (- self.target + self.output_sparsemax)/(batch_size * self.num_clusters)\n",
    "        return grad_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 273774,
     "status": "error",
     "timestamp": 1525035190570,
     "user": {
      "displayName": "Satyam Mittal",
      "photoUrl": "//lh4.googleusercontent.com/-6FSMn_ApE8Y/AAAAAAAAAAI/AAAAAAAAGe0/v35Ajvas9Js/s50-c-k-no/photo.jpg",
      "userId": "109877607729935346899"
     },
     "user_tz": -330
    },
    "id": "X0JekMAIQZHA",
    "outputId": "7d956a74-9ccc-4ba3-a014-781cb34878db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [59000/60000 (98.333%)]\tLoss: 0.284\n",
      "\n",
      "Train Epoch: 2 [59000/60000 (98.333%)]\tLoss: 0.179\n",
      "\n",
      "Train Epoch: 3 [59000/60000 (98.333%)]\tLoss: 0.141\n",
      "\n",
      "Train Epoch: 4 [59000/60000 (98.333%)]\tLoss: 0.145\n",
      "\n",
      "Train Epoch: 5 [59000/60000 (98.333%)]\tLoss: 0.080\n",
      "\n",
      "Train Epoch: 6 [59000/60000 (98.333%)]\tLoss: 0.123\n",
      "\n",
      "Train Epoch: 7 [59000/60000 (98.333%)]\tLoss: 0.077\n",
      "\n",
      "Train Epoch: 8 [31000/60000 (51.667%)]\tLoss: 0.065Train Epoch: 8 [59000/60000 (98.333%)]\tLoss: 0.110\n",
      "\n",
      "Train Epoch: 9 [59000/60000 (98.333%)]\tLoss: 0.115\n",
      "\n",
      "Train Epoch: 10 [59000/60000 (98.333%)]\tLoss: 0.076\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7a043da97308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-7a043da97308>\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Variable data has to be a tensor, but got Variable"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import random\n",
    "import numpy as np\n",
    "from random import randint\n",
    "class Parse:\n",
    "  batch_size=100\n",
    "  softmax=0\n",
    "  test_batch_sze=100\n",
    "  epochs=10\n",
    "  lr=0.01\n",
    "  momentum=0.5\n",
    "  cuda=False\n",
    "  seed=1\n",
    "  log_interval=10\n",
    "\n",
    "\n",
    "# Training settings\n",
    "#parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "#parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    " #                   help='input batch size for training (default: 64)')\n",
    "#parser.add_argument('--softmax', type=bool, default=0, metavar='N',\n",
    "  #                  help='For switching to softmax')\n",
    "#parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    " #                   help='input batch size for testing (default: 1000)')\n",
    "#parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "#                    help='number of epochs to train (default: 2)')\n",
    "#parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                    help='learning rate (default: 0.01)')\n",
    "#parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                    help='SGD momentum (default: 0.5)')\n",
    "#parser.add_argument('--cuda', action='store_true', default=False,\n",
    "#                    help='disables CUDA training')\n",
    "#parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                    help='random seed (default: 1)')\n",
    "#parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                    help='how many batches to wait before logging training status')\n",
    "args = Parse()\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def generateRandomCliqueVector(clusters, nodes_per_cluster):\n",
    "\tresult = np.zeros(clusters*nodes_per_cluster)\n",
    "\tfor i in range(clusters):\n",
    "\t\tj = random.randint(0,nodes_per_cluster-1)\n",
    "\t\tresult[i*nodes_per_cluster+j]=1.0\n",
    "\treturn result\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, H_clusters, H_neurons_per_cluster):\n",
    "        super(Net, self).__init__()\n",
    "        self.H_clusters=H_clusters\n",
    "        self.H_neurons_per_cluster=H_neurons_per_cluster\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50,self.H_clusters*self.H_neurons_per_cluster)\n",
    "\n",
    "\n",
    "        self.sparsemaxActivation = Sparsemax(self.H_clusters,self.H_neurons_per_cluster)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        if args.softmax:\n",
    "             return F.log_softmax(x, dim=1)\n",
    "        else:\n",
    "            y_pred, zs_sparse, taus, is_gt = self.sparsemaxActivation(x)\n",
    "            return x, y_pred, zs_sparse, taus, is_gt\n",
    "\n",
    "H_clusters, H_neurons_per_cluster, N_class = 1, 10, 10\n",
    "model = Net(H_clusters, H_neurons_per_cluster)\n",
    "sparsemaxMulticlassLoss = MultiLabelSparseMaxLoss(H_clusters, H_neurons_per_cluster)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "code_target_class = np.zeros((N_class,H_clusters*H_neurons_per_cluster), dtype='float32')\n",
    "\n",
    "for i in range(N_class):\n",
    "    one_hot_vector = np.zeros(H_clusters*H_neurons_per_cluster)\n",
    "\t#code_target_class[i] = generateRandomCliqueVector(H_clusters,H_neurons_per_cluster).reshape((H_clusters*H_neurons_per_cluster))\n",
    "    one_hot_vector[i] = 1.0\n",
    "    code_target_class[i]=one_hot_vector\n",
    "\n",
    "table_embedding = nn.Embedding(N_class, H_clusters*H_neurons_per_cluster, sparse=True)\n",
    "table_embedding.volatile=True\n",
    "table_embedding.requires_grad=False\n",
    "table_embedding.weight = nn.Parameter(torch.from_numpy(code_target_class))\n",
    "table_embedding.weight.requires_grad=False\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        code_target = table_embedding(target)\n",
    "        optimizer.zero_grad()\n",
    "        #print (model(data))\n",
    "        if args.softmax:\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "        else:\n",
    "            input_sparsemax, y_pred, zs_sparse, taus, is_gt = model(data)\n",
    "            loss = sparsemaxMulticlassLoss(input_sparsemax, zs_sparse, code_target, y_pred, taus, is_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:.3f}%)]\\tLoss: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]), end='')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if args.softmax:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        else:\n",
    "            #data, target = Variable(data, volatile=True), Variable(target)\n",
    "            _, output, _ , _ , _ = model(data)\n",
    "        pred = output.data.max(1)[1]\n",
    "\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\rTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)),end = '')\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    print('\\n')\n",
    "print('\\n')\n",
    "test()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6fVXerYMQkyK"
   },
   "outputs": [],
   "source": [
    "class Parse:\n",
    "  def __init__(self):\n",
    "    self.batchsize=64\n",
    "    self.softmax=1\n",
    "    self.testbatchsize=1000\n",
    "    self.epochs=2\n",
    "    self.lr=0.01\n",
    "    self.momentum=0.5\n",
    "    self.cude=True\n",
    "    self.seed=1\n",
    "    self.loginterval=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1525033777030,
     "user": {
      "displayName": "Satyam Mittal",
      "photoUrl": "//lh4.googleusercontent.com/-6FSMn_ApE8Y/AAAAAAAAAAI/AAAAAAAAGe0/v35Ajvas9Js/s50-c-k-no/photo.jpg",
      "userId": "109877607729935346899"
     },
     "user_tz": -330
    },
    "id": "atncR0Gjb408",
    "outputId": "3fbb18b8-b6ac-4aa9-8dbf-4fc8fac14c3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1525034567810,
     "user": {
      "displayName": "Satyam Mittal",
      "photoUrl": "//lh4.googleusercontent.com/-6FSMn_ApE8Y/AAAAAAAAAAI/AAAAAAAAGe0/v35Ajvas9Js/s50-c-k-no/photo.jpg",
      "userId": "109877607729935346899"
     },
     "user_tz": -330
    },
    "id": "Pb8PpZ71GBkE",
    "outputId": "cbca8408-2f33-48b5-b4fc-a8110d914334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d (1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d (10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5)\n",
      "  (fc1): Linear(in_features=320, out_features=50)\n",
      "  (fc2): Linear(in_features=50, out_features=10)\n",
      "  (sparsemaxActivation): Sparsemax(\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271486,
     "status": "ok",
     "timestamp": 1525035616084,
     "user": {
      "displayName": "Satyam Mittal",
      "photoUrl": "//lh4.googleusercontent.com/-6FSMn_ApE8Y/AAAAAAAAAAI/AAAAAAAAGe0/v35Ajvas9Js/s50-c-k-no/photo.jpg",
      "userId": "109877607729935346899"
     },
     "user_tz": -330
    },
    "id": "OThqSOMVJKBa",
    "outputId": "1c5a37a2-52b9-4cc9-80c0-dd0e52d90ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [59000/60000 (98.333%)]\tLoss: 0.678\n",
      "\n",
      "Train Epoch: 2 [59000/60000 (98.333%)]\tLoss: 0.371\n",
      "\n",
      "Train Epoch: 3 [59000/60000 (98.333%)]\tLoss: 0.456\n",
      "\n",
      "Train Epoch: 4 [59000/60000 (98.333%)]\tLoss: 0.233\n",
      "\n",
      "Train Epoch: 5 [59000/60000 (98.333%)]\tLoss: 0.157\n",
      "\n",
      "Train Epoch: 6 [59000/60000 (98.333%)]\tLoss: 0.366\n",
      "\n",
      "Train Epoch: 7 [59000/60000 (98.333%)]\tLoss: 0.281\n",
      "\n",
      "Train Epoch: 8 [31000/60000 (51.667%)]\tLoss: 0.172Train Epoch: 8 [59000/60000 (98.333%)]\tLoss: 0.286\n",
      "\n",
      "Train Epoch: 9 [59000/60000 (98.333%)]\tLoss: 0.215\n",
      "\n",
      "Train Epoch: 10 [59000/60000 (98.333%)]\tLoss: 0.207\n",
      "\n",
      "Sparsemax\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0651, Accuracy: 9788/10000 (97.880%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Parse:\n",
    "  batch_size=100\n",
    "  softmax=1\n",
    "  test_batch_sze=100\n",
    "  epochs=10\n",
    "  lr=0.01\n",
    "  momentum=0.5\n",
    "  cuda=False\n",
    "  seed=1\n",
    "  log_interval=10\n",
    "\n",
    "\n",
    "# Training settings\n",
    "#parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "#parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    " #                   help='input batch size for training (default: 64)')\n",
    "#parser.add_argument('--softmax', type=bool, default=0, metavar='N',\n",
    "  #                  help='For switching to softmax')\n",
    "#parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    " #                   help='input batch size for testing (default: 1000)')\n",
    "#parser.add_argument('--epochs', type=int, default=2, metavar='N',\n",
    "#                    help='number of epochs to train (default: 2)')\n",
    "#parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "#                    help='learning rate (default: 0.01)')\n",
    "#parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "#                    help='SGD momentum (default: 0.5)')\n",
    "#parser.add_argument('--cuda', action='store_true', default=False,\n",
    "#                    help='disables CUDA training')\n",
    "#parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                    help='random seed (default: 1)')\n",
    "#parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                    help='how many batches to wait before logging training status')\n",
    "args = Parse()\n",
    "args.cuda = args.cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "def generateRandomCliqueVector(clusters, nodes_per_cluster):\n",
    "\tresult = np.zeros(clusters*nodes_per_cluster)\n",
    "\tfor i in range(clusters):\n",
    "\t\tj = random.randint(0,nodes_per_cluster-1)\n",
    "\t\tresult[i*nodes_per_cluster+j]=1.0\n",
    "\treturn result\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, H_clusters, H_neurons_per_cluster):\n",
    "        super(Net, self).__init__()\n",
    "        self.H_clusters=H_clusters\n",
    "        self.H_neurons_per_cluster=H_neurons_per_cluster\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50,self.H_clusters*self.H_neurons_per_cluster)\n",
    "\n",
    "\n",
    "        self.sparsemaxActivation = Sparsemax(self.H_clusters,self.H_neurons_per_cluster)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        if args.softmax:\n",
    "             return F.log_softmax(x, dim=1)\n",
    "        else:\n",
    "            y_pred, zs_sparse, taus, is_gt = self.sparsemaxActivation(x)\n",
    "            return x, y_pred, zs_sparse, taus, is_gt\n",
    "\n",
    "H_clusters, H_neurons_per_cluster, N_class = 1, 10, 10\n",
    "model = Net(H_clusters, H_neurons_per_cluster)\n",
    "sparsemaxMulticlassLoss = MultiLabelSparseMaxLoss(H_clusters, H_neurons_per_cluster)\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "code_target_class = np.zeros((N_class,H_clusters*H_neurons_per_cluster), dtype='float32')\n",
    "\n",
    "for i in range(N_class):\n",
    "    one_hot_vector = np.zeros(H_clusters*H_neurons_per_cluster)\n",
    "\t#code_target_class[i] = generateRandomCliqueVector(H_clusters,H_neurons_per_cluster).reshape((H_clusters*H_neurons_per_cluster))\n",
    "    one_hot_vector[i] = 1.0\n",
    "    code_target_class[i]=one_hot_vector\n",
    "\n",
    "table_embedding = nn.Embedding(N_class, H_clusters*H_neurons_per_cluster, sparse=True)\n",
    "table_embedding.volatile=True\n",
    "table_embedding.requires_grad=False\n",
    "table_embedding.weight = nn.Parameter(torch.from_numpy(code_target_class))\n",
    "table_embedding.weight.requires_grad=False\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        code_target = table_embedding(target)\n",
    "        optimizer.zero_grad()\n",
    "        #print (model(data))\n",
    "        if args.softmax:\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "        else:\n",
    "            input_sparsemax, y_pred, zs_sparse, taus, is_gt = model(data)\n",
    "            loss = sparsemaxMulticlassLoss(input_sparsemax, zs_sparse, code_target, y_pred, taus, is_gt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('\\rTrain Epoch: {} [{}/{} ({:.3f}%)]\\tLoss: {:.3f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]), end='')\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args.cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if args.softmax:\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        else:\n",
    "            #data, target = Variable(data, volatile=True), Variable(target)\n",
    "            _, output, _ , _ , _ = model(data)\n",
    "        pred = output.data.max(1)[1]\n",
    "\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\rTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)),end = '')\n",
    "\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "    print('\\n')\n",
    "print('Sparsemax')\n",
    "print('\\n')\n",
    "test()\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Z4nbCnkyL4Le"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "default_view": {},
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
